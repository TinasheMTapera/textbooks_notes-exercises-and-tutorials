---
title: "Chapter 10: Resampling for Evaluating Performance"
format: html
---

```{r, include=FALSE}
source("../ames_snippets.r")
```

You can evaluate a model on the test set, but this is not
recommended for models where you need to _tune_ your model
parameters â€” tuning params on the test set introduces _leakage_.
A better approach is to _resample_ your training set. Tidymodels
provides great functions for doing this.

Here, we fit a new model to the `ames` dataset: a random forest.
Notice that we start a new workflow object here:

```{r}
rf_model <- 
  rand_forest(trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("regression")

rf_wflow <- 
  workflow() %>% 
  add_formula(
    Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
      Latitude + Longitude) %>% 
  add_model(rf_model) 

rf_fit <- rf_wflow %>% fit(data = ames_train)
```

Now, compare the `lm` workflow agains the `ranger::random_forest` workflow with tidymodels:

```{r}
estimate_perf <- function(model, dat) {
    # Capture the names of the `model` and `dat` objects
    cl <- match.call()
    obj_name <- as.character(cl$model)
    data_name <- as.character(cl$dat)
    data_name <- gsub("ames_", "", data_name)

    reg_metrics <- metric_set(rmse, rsq)
      model %>%
    predict(dat) %>%
    bind_cols(dat %>% select(Sale_Price)) %>%
    reg_metrics(Sale_Price, .pred) %>%
    select(-.estimator) %>%
    mutate(object = obj_name, data = data_name)
}

bind_rows(list(Random_forest=estimate_perf(rf_fit, ames_train), LM=estimate_perf(lm_fit, ames_train)), .id="id")
```

So, Random Forest is clearly superior, right? Well, let's see
how it performs on the test set:

```{r}
estimate_perf(rf_fit, ames_test)
```

The RMSE is actually _higher_! How could this happen? Well,
in statistics, _low bias_ models are those that closely
approximate the data. In this case, the bias was so low
that the training data was basically memorised.

The main takeaway from this example is that repredicting the training set will result in an artificially optimistic estimate of performance. It is a bad idea for most models. Instead, utilise resampling.

![Resampling illustrated](https://www.tmwr.org/premade/resampling.svg)

Basically, the model is fit on the _analysis_ set, and tested on the _assessment_ set, repeatedly (this language helps differentiate this process from training and testing).

## Resampling Methods

### 1. Cross Validation

For each iteration, one fold is held out for assessment statistics and the remaining folds are substrate for the model. This process continues for each fold so that three models produce three sets of performance statistics.

Kuhn et al. recommend 10 folds to balance bias and variance.

You can easily create folds with `tidymodels`

```{r}
ames_folds <- vfold_cv(ames_train, v = 10)
ames_folds
```
