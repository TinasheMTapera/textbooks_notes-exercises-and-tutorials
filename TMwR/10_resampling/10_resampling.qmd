---
title: "Chapter 10: Resampling for Evaluating Performance"
format: html
---

```{r, include=FALSE}
source("../ames_snippets.r")
```

You can evaluate a model on the test set, but this is not
recommended for models where you need to _tune_ your model
parameters â€” tuning params on the test set introduces _leakage_.
A better approach is to _resample_ your training set. Tidymodels
provides great functions for doing this.

Here, we fit a new model to the `ames` dataset: a random forest.
Notice that we start a new workflow object here:

```{r}
rf_model <- 
  rand_forest(trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("regression")

rf_wflow <- 
  workflow() %>% 
  add_formula(
    Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
      Latitude + Longitude) %>% 
  add_model(rf_model) 

rf_fit <- rf_wflow %>% fit(data = ames_train)
```

Now, compare the `lm` workflow agains the `ranger::random_forest` workflow with tidymodels:

```{r}
estimate_perf <- function(model, dat) {
    # Capture the names of the `model` and `dat` objects
    cl <- match.call()
    obj_name <- as.character(cl$model)
    data_name <- as.character(cl$dat)
    data_name <- gsub("ames_", "", data_name)

    reg_metrics <- metric_set(rmse, rsq)
      model %>%
    predict(dat) %>%
    bind_cols(dat %>% select(Sale_Price)) %>%
    reg_metrics(Sale_Price, .pred) %>%
    select(-.estimator) %>%
    mutate(object = obj_name, data = data_name)
}

bind_rows(list(Random_forest=estimate_perf(rf_fit, ames_train), LM=estimate_perf(lm_fit, ames_train)), .id="id")
```

So, Random Forest is clearly superior, right? Well, let's see
how it performs on the test set:

```{r}
estimate_perf(rf_fit, ames_test)
```

The RMSE is actually _higher_! How could this happen? Well,
in statistics, _low bias_ models are those that closely
approximate the data. In this case, the bias was so low
that the training data was basically memorised.

The main takeaway from this example is that repredicting the training set will result in an artificially optimistic estimate of performance. It is a bad idea for most models. Instead, utilise resampling.

![Resampling illustrated](https://www.tmwr.org/premade/resampling.svg)

Basically, the model is fit on the _analysis_ set, and tested on the _assessment_ set, repeatedly (this language helps differentiate this process from training and testing).

## Resampling Methods

### 1. Cross Validation

For each iteration, one fold is held out for assessment statistics and the remaining folds are substrate for the model. This process continues for each fold so that three models produce three sets of performance statistics.

Kuhn et al. recommend 10 folds to balance bias and variance.

You can easily create folds with `tidymodels`

```{r}
ames_folds <- vfold_cv(ames_train, v = 10)
ames_folds
```

In order to access each fold's analysis or assessment subset, use the functions:

```{r}
ames_folds$splits[[1]] %>% 
  analysis() %>% 
  dim()
```

There are other methods of cross validation available though:

### 2. Repeated CV

You can easily create repeated folds with the same strategy:


```{r}
vfold_cv(ames_train, v = 10, repeats = 5)
```

### 3. LOOCV

You can use the function `loo_cv()` to get leave-one-out cross-validation.

### 4. Monte-Carlo CV

MCCV is similar to repeated CV but uses the assessment set as the target,
selecting a fixed proportion of data at a time randomly. This means that
each assessment set does not have to be mutually exclusive.


```{r}
mc_cv(ames_train, prop = 9/10, times = 20)
```

### 5. Validation Set

You can use a simple validation set split when you have sufficient data to not
worry about tuning over multiple folds ![Validation](https://www.tmwr.org/premade/validation-alt.svg)

### 6. Bootstrapping

Bootstrapping is similar to CV but samples are drawn with replacement, meaning
that some training set data points are selected multiple times for the analysis set.

![bootstrap](https://www.tmwr.org/premade/bootstraps.svg)

It's similarly easy:


```{r}
bootstraps(ames_train, times = 5)
```

### 7. Rolling Forecasting Origin Sampling

This is a special kind of resampling that ensures to respect
time-based correlation in the resampling approach.

![Forecast sampling](https://www.tmwr.org/premade/rolling.svg)


