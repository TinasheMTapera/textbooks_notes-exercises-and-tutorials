[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tidy Modeling with R: Notes & Exercises",
    "section": "",
    "text": "This is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "01_software_for_modeling/01_software_for_modeling.html",
    "href": "01_software_for_modeling/01_software_for_modeling.html",
    "title": "1Â  Chapter 1: Software for Modeling",
    "section": "",
    "text": "Note\n\n\n\nDefinition: Models are mathematical tools that can describe a system and capture relationships in the data given to them"
  },
  {
    "objectID": "01_software_for_modeling/01_software_for_modeling.html#types-of-models",
    "href": "01_software_for_modeling/01_software_for_modeling.html#types-of-models",
    "title": "1Â  Chapter 1: Software for Modeling",
    "section": "1.1 Types of Models",
    "text": "1.1 Types of Models\n\nDescriptive models: To describe or illustrate characteristics of data. e.g.Â visually emphasise a trend or artifact\nInferential models: To produce a decision for a research question or to explore a specific hypothesis, similar to how statistical tests are used (remember t-tests & ANOVA are GLMs)\n\nInferential mode starts with an assumption about a population, then produces a statistical conclusion to confirm or reject that assumption.\n\n\n\n\n\n\nNote\n\n\n\nUltimately, a model for statistical inference posits: â€œIf my data were truly independent (unbiased), and we fit a model to it, and the residuals of that model follow a known distribution X, then test statistic Y can be used to produce a p-value. Otherwise, the resulting p-value might be inaccurate.â€\n\n\n\nPredictive models: The primary goal is that the predicted values have the highest possible fidelity to the true value of the new data.\n\nThere are two approaches â€” mechanistic (known mechanisms of first principles a priori, like knowing how to model effort by knowing itâ€™s a combination of reward and risk) or empirical, where the model is defined only by the structure of the prediction\nThese can be lumped into supervised (known outcome, e.g.Â regression or classification) and unsupervised (no known outcome, e.g.Â pattern or cluster analysis)"
  },
  {
    "objectID": "03_review_modeling_fundamentals/03_review_modeling_fundamentals.html",
    "href": "03_review_modeling_fundamentals/03_review_modeling_fundamentals.html",
    "title": "2Â  Ch. 3: A Review of R Modeling Fundamentals",
    "section": "",
    "text": "Letâ€™s look at the crickets data."
  },
  {
    "objectID": "03_review_modeling_fundamentals/03_review_modeling_fundamentals.html#tidymodels-goals",
    "href": "03_review_modeling_fundamentals/03_review_modeling_fundamentals.html#tidymodels-goals",
    "title": "2Â  Ch. 3: A Review of R Modeling Fundamentals",
    "section": "2.1 Tidymodels goals",
    "text": "2.1 Tidymodels goals\nTo unify modeling under the tidy design guide:\n\nCode adheres to the tidy structure for inputs, defaults, dots, outputs, errors, and side effects\n\n\n\nIt is human centered, i.e.Â the tidyverse is designed specifically to support the activities of a human data analyst.\nIt is consistent, so that what you learn about one function or package can be applied to another, and the number of special cases that you need to remember is as small as possible.\nIt is composable, allowing you to solve complex problems by breaking them down into small pieces, supporting a rapid cycle of exploratory iteration to find the best solution.\nIt is inclusive, because the tidyverse is not just the collection of packages, but it is also the community of people who use them."
  },
  {
    "objectID": "04_ames_housing_data/04_ames_housing_data.html",
    "href": "04_ames_housing_data/04_ames_housing_data.html",
    "title": "3Â  Ch. 4: Introduction to Ames Housing Data",
    "section": "",
    "text": "Load the data"
  },
  {
    "objectID": "04_ames_housing_data/04_ames_housing_data.html#some-eda",
    "href": "04_ames_housing_data/04_ames_housing_data.html#some-eda",
    "title": "3Â  Ch. 4: Introduction to Ames Housing Data",
    "section": "3.1 Some EDA",
    "text": "3.1 Some EDA\nFirst examine the outcome variable:\n\nlibrary(tidymodels)\n\nâ”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels 1.0.0 â”€â”€\n\n\nâœ” broom        1.0.0     âœ” rsample      1.1.0\nâœ” dials        1.0.0     âœ” tibble       3.1.8\nâœ” dplyr        1.0.9     âœ” tidyr        1.2.0\nâœ” ggplot2      3.3.6     âœ” tune         1.0.0\nâœ” infer        1.0.3     âœ” workflows    1.0.0\nâœ” parsnip      1.0.1     âœ” workflowsets 1.0.0\nâœ” purrr        0.3.4     âœ” yardstick    1.0.0\nâœ” recipes      1.0.1     \n\n\nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels_conflicts() â”€â”€\nâœ– purrr::discard() masks scales::discard()\nâœ– dplyr::filter()  masks stats::filter()\nâœ– dplyr::lag()     masks stats::lag()\nâœ– recipes::step()  masks stats::step()\nâ€¢ Learn how to get started at https://www.tidymodels.org/start/\n\names %>%\n    ggplot(aes(x=Sale_Price)) +\n        geom_histogram(bins=50)\n\n\n\n\nWhat are the quantiles and median?\n\nsummary(ames$Sale_Price)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  12789  129500  160000  180796  213500  755000 \n\n\nShow median:\n\names %T>%\n    {m <<- .$Sale_Price %>% median() } %>%\n    ggplot() +\n        geom_histogram(aes(x=Sale_Price), bins=50) +\n        geom_vline(xintercept = m, colour = \"red\")\n\n\n\n\nRight skewed outcome, so log transforming will be recommended:\n\names %>%\n    ggplot(aes(x=Sale_Price)) +\n        geom_histogram(bins=50) +\n        scale_x_log10()\n\n\n\n\nMore normal, but with the disadvantage that interpretability may be lost.\n\n# move forward with this decision\names <- ames %>% mutate(Sale_Price = log10(Sale_Price))\n\nHereâ€™s a quick EDA checklist:\n\nIdentify the outcome variable\n\nWhat kind of distribution does it have? Is the distribution appropriate for modeling/ML?\nIf categorical, are the classes balanced?\nIs it skewed?\nAre there outliers? How severe are they?\n\nFor each predictor variable:\n\nWhat kind of distribution does it have/what is the count of categories?\nDo predictors autocorrelate?\nDoes the predictor correlate with the outcome?"
  },
  {
    "objectID": "05_spending_data/05_spending_data.html",
    "href": "05_spending_data/05_spending_data.html",
    "title": "4Â  Ch. 5: Spending our Data",
    "section": "",
    "text": "A common first step is â€œsplittingâ€ data into a training and testing set.\nExample using rsample to get an 80-20 split:\n\nlibrary(tidymodels)\nset.seed(501)\n\names_split <- initial_split(ames, prop=.8)\n\names_split\n\n<Training/Testing/Total>\n<2344/586/2930>\n\n\nUse the functions below to get your data into split dfs:\n\names_train <- training(ames_split)\names_test <- testing(ames_split)\n\nIn the case of classification with imbalanced classes, stratified classification can be used to do splitting separately for each class and then combining the result. For regression, a similar approach is to stratify by quantiles:\n\nqs <- quantile(ames$Sale_Price)[2:4]\names %>%\n    ggplot(aes(x=Sale_Price)) +\n        geom_histogram(bins=50) +\n        geom_vline(xintercept = qs, lty=\"dashed\")\n\n\n\n\nWithin each, it will run the sampling for train and test :)\n\names_split <- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train <- training(ames_split)\names_test  <-  testing(ames_split)\n\ndim(ames_train)\n\n[1] 2342   74\n\n\n\n\n\n\n\n\nTip\n\n\n\nThere is very little downside to using stratified sampling\n\n\nNote that for time series data, you can use initial_time_split() to create a train test split; this assumes that you want later data to be forecasted and earlier data to be the training data.\nItâ€™s also important to separate your notion of a test set from a validation set. A validation set is what you use to validate your training models, i.e.Â you use it to tune parameters. A test set must remain an unbiased (unseen) data set that the final model is evaluated on.\n\n4.0.1 Multilevel Modeling\nWhen data points in the outcome are structurally or conceptually related to each other, this might be a case for multilevel modeling. e.g.Â longitudinal data where one participant has multiple time points/repeated measures (or a mobile phone study where one participant has many EMA responses ğŸ˜‰)\n\n\n4.0.2 Other considerations\n\nkeep your test set far away from your training set\nno matter your operations on the data, keep your test set as close to real world data as possible\nThese considerations are for training and evaluation of the model; once you know the model is the best it can be, you are free to use the full dataset to productionize the model"
  },
  {
    "objectID": "06_model_fitting_parsnip/06_model_fitting_parsnip.html",
    "href": "06_model_fitting_parsnip/06_model_fitting_parsnip.html",
    "title": "5Â  Chapter 6: Fitting Models with parsnip",
    "section": "",
    "text": "Specify the type of model based on its mathematical structure (e.g., linear regression, random forest, KNN, etc).\nSpecify the engine for fitting the model. Most often this reflects the software package that should be used, like Stan or glmnet. These are models in their own right, and parsnip provides consistent interfaces by using these as engines for modeling.\nWhen required, declare the mode of the model. The mode reflects the type of prediction outcome. For numeric outcomes, the mode is regression; for qualitative outcomes, it is classification.13 If a model algorithm can only address one type of prediction outcome, such as linear regression, the mode is already set.\n\nFor eg:\n\nlibrary(tidymodels)\n\nlinear_reg() %>% set_engine(\"lm\") %>% translate()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\nModel fit template:\nstats::lm(formula = missing_arg(), data = missing_arg(), weights = missing_arg())\n\n\nPretty cool!\nNow to fit some data to ames:\n\nlm_model <- \n  linear_reg() %>% \n  set_engine(\"lm\")\n\n# for a formula interface\nlm_form_fit <- lm_model %>%\n    fit(Sale_Price ~ Longitude + Latitude, data=ames_train)\n\n# for an X= Y= interface\nlm_xy_fit <- lm_model %>%\n    fit_xy(\n        x = ames_train %>% select(Longitude, Latitude),\n        y = ames_train %>% pull(Sale_Price)\n    )\n\n#they are identical\nlm_xy_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)    Longitude     Latitude  \n   -302.974       -2.075        2.710  \n\n\nIn reference to standardisation, parsnip goes out of its way to ensure names are consistent across models:\n\nIf a practitioner were to include these names in a plot or table, would the people viewing those results understand the name?\n\nUse translate() to understand how a parsnip model getâ€™s translated to a library:\n\nrand_forest(trees = 1000, min_n = 5) %>% \n  set_engine(\"ranger\") %>% \n  set_mode(\"regression\") %>% \n  translate()\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  trees = 1000\n  min_n = 5\n\nComputational engine: ranger \n\nModel fit template:\nranger::ranger(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n    num.trees = 1000, min.node.size = min_rows(~5, x), num.threads = 1, \n    verbose = FALSE, seed = sample.int(10^5, 1))\n\n\nIt maintains two types of arguments, main arguments which are common across one or more models, and engine arguments which only pertain to the particular model/libary; e.g to make ranger do verbose, set it in the engine (not in parsnipâ€™s main args):\n\nrand_forest(trees = 1000, min_n = 5) %>% \n  set_engine(\"ranger\", verbose = TRUE) %>% \n  set_mode(\"regression\") %>%\n  translate()\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  trees = 1000\n  min_n = 5\n\nEngine-Specific Arguments:\n  verbose = TRUE\n\nComputational engine: ranger \n\nModel fit template:\nranger::ranger(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n    num.trees = 1000, min.node.size = min_rows(~5, x), verbose = TRUE, \n    num.threads = 1, seed = sample.int(10^5, 1))\n\n\nThen, use the handy broom package:\n\ntidy(lm_form_fit)\n\n# A tibble: 3 Ã— 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)  -303.      14.4       -21.0 3.64e-90\n2 Longitude      -2.07     0.129     -16.1 1.40e-55\n3 Latitude        2.71     0.180      15.0 9.29e-49\n\n\nNext, to predict, use the whole fit object and some sample data with base Râ€™s predict():\n\names_test_small <- ames_test %>% slice(1:5)\npredict(lm_form_fit, new_data = ames_test_small)\n\n# A tibble: 5 Ã— 1\n  .pred\n  <dbl>\n1  5.22\n2  5.21\n3  5.28\n4  5.27\n5  5.28\n\n\nTidymodels makes it easy to translate your work to other models from other packages. Hereâ€™s the same workflow for a decision tree:\n\ntree_model <- \n  decision_tree(min_n = 2) %>% \n  set_engine(\"rpart\") %>% \n  set_mode(\"regression\")\n\ntree_fit <- \n  tree_model %>% \n  fit(Sale_Price ~ Longitude + Latitude, data = ames_train)\n\names_test_small %>% \n  select(Sale_Price) %>% \n  bind_cols(predict(tree_fit, ames_test_small))\n\n# A tibble: 5 Ã— 2\n  Sale_Price .pred\n       <dbl> <dbl>\n1       5.02  5.15\n2       5.39  5.15\n3       5.28  5.32\n4       5.28  5.32\n5       5.28  5.32"
  },
  {
    "objectID": "07_model_workflow/07_model_workflow.html",
    "href": "07_model_workflow/07_model_workflow.html",
    "title": "6Â  Chapter 7: A Model Workflow",
    "section": "",
    "text": "Itâ€™s important to consider where the â€œmodelingâ€ concept begins and ends. For eg PCA is part of modeling. tidymodels recommends bundling model components into â€œworkflowsâ€ with the workflows package.how different is this from targets?\nCreate a workflow object:\nThe model has to be something from parsnip. Next, Add the model formula:\nThen fit the model on training data:\nPredict is easy:\nAlternately, you can provide outcome and predictors using tidyselect methods:\nInstead of using add_formula(), you can add the formula at the same time as you add the model. This is useful to get around the weirdness of different libraries requiring different model specifications:"
  },
  {
    "objectID": "07_model_workflow/07_model_workflow.html#specifying-multiple-models",
    "href": "07_model_workflow/07_model_workflow.html#specifying-multiple-models",
    "title": "6Â  Chapter 7: A Model Workflow",
    "section": "6.1 Specifying multiple models",
    "text": "6.1 Specifying multiple models\nHereâ€™s how to specify multiple models at once using the workflowset library:\n\nlocation <- list(\n  longitude = Sale_Price ~ Longitude,\n  latitude = Sale_Price ~ Latitude,\n  coords = Sale_Price ~ Longitude + Latitude,\n  neighborhood = Sale_Price ~ Neighborhood\n)\n\nlibrary(workflowsets)\nlocation_models <- workflow_set(preproc = location, models = list(lm = lm_model))\nlocation_models\n\n# A workflow set/tibble: 4 Ã— 4\n  wflow_id        info             option    result    \n  <chr>           <list>           <list>    <list>    \n1 longitude_lm    <tibble [1 Ã— 4]> <opts[0]> <list [0]>\n2 latitude_lm     <tibble [1 Ã— 4]> <opts[0]> <list [0]>\n3 coords_lm       <tibble [1 Ã— 4]> <opts[0]> <list [0]>\n4 neighborhood_lm <tibble [1 Ã— 4]> <opts[0]> <list [0]>\n\n\nInspect:\n\nlocation_models$info[[1]]\n\n# A tibble: 1 Ã— 4\n  workflow   preproc model      comment\n  <list>     <chr>   <chr>      <chr>  \n1 <workflow> formula linear_reg \"\"     \n\n\nPull:\n\nextract_workflow(location_models, id = \"coords_lm\")\n\nâ•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nPreprocessor: Formula\nModel: linear_reg()\n\nâ”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nSale_Price ~ Longitude + Latitude\n\nâ”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nNow to run, we use purrr::map\n\nlocation_models <-\n   location_models %>%\n   mutate(fit = map(info, ~ fit(.x$workflow[[1]], ames_train)))\nlocation_models\n\n# A workflow set/tibble: 4 Ã— 5\n  wflow_id        info             option    result     fit       \n  <chr>           <list>           <list>    <list>     <list>    \n1 longitude_lm    <tibble [1 Ã— 4]> <opts[0]> <list [0]> <workflow>\n2 latitude_lm     <tibble [1 Ã— 4]> <opts[0]> <list [0]> <workflow>\n3 coords_lm       <tibble [1 Ã— 4]> <opts[0]> <list [0]> <workflow>\n4 neighborhood_lm <tibble [1 Ã— 4]> <opts[0]> <list [0]> <workflow>\n\n\n\nlocation_models$fit[[1]]\n\nâ•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nPreprocessor: Formula\nModel: linear_reg()\n\nâ”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nSale_Price ~ Longitude\n\nâ”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)    Longitude  \n   -184.396       -2.025  \n\n\nFinally, you can use last_fit() to wrap the whole process, a workflow + fit + prediction, using the initial split object:\n\nfinal_lm_res <- last_fit(lm_wflow, ames_split)\nfinal_lm_res\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 Ã— 6\n  splits             id               .metrics .notes   .predictions .workflow \n  <list>             <chr>            <list>   <list>   <list>       <list>    \n1 <split [2342/588]> train/test split <tibble> <tibble> <tibble>     <workflow>\n\n\n\nfitted_lm_wflow <- extract_workflow(final_lm_res)\ncollect_metrics(final_lm_res)\n\n# A tibble: 2 Ã— 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       0.164 Preprocessor1_Model1\n2 rsq     standard       0.189 Preprocessor1_Model1\n\ncollect_predictions(final_lm_res) %>% slice(1:5)\n\n# A tibble: 5 Ã— 5\n  id               .pred  .row Sale_Price .config             \n  <chr>            <dbl> <int>      <dbl> <chr>               \n1 train/test split  5.22     2       5.02 Preprocessor1_Model1\n2 train/test split  5.21     4       5.39 Preprocessor1_Model1\n3 train/test split  5.28     5       5.28 Preprocessor1_Model1\n4 train/test split  5.27     8       5.28 Preprocessor1_Model1\n5 train/test split  5.28    10       5.28 Preprocessor1_Model1"
  },
  {
    "objectID": "08_feat_eng_recipes/08_feat_eng_recipes.html",
    "href": "08_feat_eng_recipes/08_feat_eng_recipes.html",
    "title": "7Â  Chapter 8: Feature Engineering with Recipes",
    "section": "",
    "text": "Weâ€™ll use the recipes package to develop feature engineering pipelines.\nBelow, define a recipe object and then add steps to it:\n\nlibrary(tidymodels) # Includes the recipes package\ntidymodels_prefer()\n\nsimple_ames <- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type,\n         data = ames_train) %>%\n  step_log(Gr_Liv_Area, base = 10) %>% \n  step_dummy(all_nominal_predictors())\n\n\nsimple_ames\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          4\n\nOperations:\n\nLog transformation on Gr_Liv_Area\nDummy variables from all_nominal_predictors()\n\n\nWe can add this recipe to our previous workflow:\n\nlm_wflow <- \n  lm_wflow %>% \n  remove_variables() %>% \n  add_recipe(simple_ames)\n\n\nlm_wflow\n\nâ•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nPreprocessor: Recipe\nModel: linear_reg()\n\nâ”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n2 Recipe Steps\n\nâ€¢ step_log()\nâ€¢ step_dummy()\n\nâ”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nNow the preprocessor step is a recipe! We can execute the recipe and model with fit() as before:\n\nlm_fit <- fit(lm_wflow, ames_train)\npredict(lm_fit, ames_test %>% slice(1:3))\n\nWarning in predict.lm(object = object$fit, newdata = new_data, type =\n\"response\"): prediction from a rank-deficient fit may be misleading\n\n\n# A tibble: 3 Ã— 1\n  .pred\n  <dbl>\n1  5.08\n2  5.32\n3  5.28\n\n\nYou can extract the recipe after the model has been fitted:\n\nlm_fit %>% \n  extract_recipe(estimated = TRUE)\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          4\n\nTraining data contained 2342 data points and no missing data.\n\nOperations:\n\nLog transformation on Gr_Liv_Area [trained]\nDummy variables from Neighborhood, Bldg_Type [trained]\n\nlm_fit %>% \n  # This returns the parsnip object:\n  extract_fit_parsnip() %>% \n  # Now tidy the linear model object:\n  tidy() %>% \n  slice(1:5)\n\n# A tibble: 5 Ã— 5\n  term                       estimate std.error statistic   p.value\n  <chr>                         <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)                -0.669    0.231        -2.90 3.80e-  3\n2 Gr_Liv_Area                 0.620    0.0143       43.2  2.63e-299\n3 Year_Built                  0.00200  0.000117     17.1  6.16e- 62\n4 Neighborhood_College_Creek  0.0178   0.00819       2.17 3.02e-  2\n5 Neighborhood_Old_Town      -0.0330   0.00838      -3.93 8.66e-  5\n\n\n\n7.0.1 Recipe steps\nYou can see the exhaustive list of recipe steps here. Here are annotated examples:\n\nsimple_ames <- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type,\n         data = ames_train) %>%\n    \n    # log transform step\n  step_log(Gr_Liv_Area, base = 10) %>%  \n    # assign factors with few counts to \"other\"\n  step_other(Neighborhood, threshold = 0.01) %>% \n    # create dummy variables (one hot encoding)\n  step_dummy(all_nominal_predictors()) %>%\n    # create an interaction between gr_liv_are and each one hot encoded Bldg_Type\n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %>%\n    # add a natural spline to a feature\n  step_ns(Latitude, deg_free = 20) %>%\n    # use PCA to create new, uncorrelated features \n    # from all the variables representing \"size\"\n  step_pca(matches(\"(SF$)|(Gr_Liv)\"))\n\nYou can use recipes to â€œregisterâ€ your model in R, so that the pipeline can be reproduced and rerun at any time."
  },
  {
    "objectID": "09_judging_models/09_judging_models.html",
    "href": "09_judging_models/09_judging_models.html",
    "title": "8Â  Chapter 9: Judging Model Effectiveness",
    "section": "",
    "text": "This chapter is all about evaluating how a model performs. Picking an appropriate metric is important as illustrated by .\nAn inferential model is used primarily to understand relationships, and typically emphasizes the choice (and validity) of probabilistic distributions and other generative qualities that define the model. For a model used primarily for prediction, by contrast, predictive strength is of primary importance and other concerns about underlying statistical qualities may be less important\nGenerally, Kuhn and Silge recommend that even if your primary work is developing inferential models, you should still consider predictive strength as a useful model performance metric.\n\nâ€¦optimization of statistical characteristics of the model does not imply that the model fits the data well. Even for purely inferential models, some measure of fidelity to the data should accompany the inferential results. Using this, the consumers of the analyses can calibrate their expectations of the results.\n\nTidymodels recommends model evaluation with the yardstick package.\nLetâ€™s make predictions with ames:\n\names_test_res <- predict(lm_fit, new_data = ames_test %>% select(-Sale_Price))\names_test_res\n\n# A tibble: 588 Ã— 1\n   .pred\n   <dbl>\n 1  5.07\n 2  5.31\n 3  5.28\n 4  5.33\n 5  5.30\n 6  5.24\n 7  5.67\n 8  5.52\n 9  5.34\n10  5.00\n# â€¦ with 578 more rows\n\n\n\names_test_res <- bind_cols(ames_test_res, ames_test %>% select(Sale_Price))\names_test_res\n\n# A tibble: 588 Ã— 2\n   .pred Sale_Price\n   <dbl>      <dbl>\n 1  5.07       5.02\n 2  5.31       5.39\n 3  5.28       5.28\n 4  5.33       5.28\n 5  5.30       5.28\n 6  5.24       5.26\n 7  5.67       5.73\n 8  5.52       5.60\n 9  5.34       5.32\n10  5.00       4.98\n# â€¦ with 578 more rows\n\n\nWe can simply plot to evaluate the data fidelity:\n\nggplot(ames_test_res, aes(x = Sale_Price, y = .pred)) + \n  # Create a diagonal line:\n  geom_abline(lty = 2) + \n  geom_point(alpha = 0.5) + \n  labs(y = \"Predicted Sale Price (log10)\", x = \"Sale Price (log10)\") +\n  # Scale and size the x- and y-axis uniformly:\n  coord_obs_pred()\n\n\n\n\nHereâ€™s the RMSE for this prediction:\n\nrmse(ames_test_res, truth = Sale_Price, estimate = .pred)\n\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard      0.0736\n\n\nIn yardstick, you can derive multiple metrics at once with metric_set():\n\names_metrics <- metric_set(rmse, rsq, mae)\names_metrics(ames_test_res, truth = Sale_Price, estimate = .pred)\n\n# A tibble: 3 Ã— 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard      0.0736\n2 rsq     standard      0.836 \n3 mae     standard      0.0549\n\n\nFor binary classification, the case is quite similar:\n\ndata(two_class_example)\ntibble(two_class_example)\n\n# A tibble: 500 Ã— 4\n   truth   Class1   Class2 predicted\n   <fct>    <dbl>    <dbl> <fct>    \n 1 Class2 0.00359 0.996    Class2   \n 2 Class1 0.679   0.321    Class1   \n 3 Class2 0.111   0.889    Class2   \n 4 Class1 0.735   0.265    Class1   \n 5 Class2 0.0162  0.984    Class2   \n 6 Class1 0.999   0.000725 Class1   \n 7 Class1 0.999   0.000799 Class1   \n 8 Class1 0.812   0.188    Class1   \n 9 Class2 0.457   0.543    Class2   \n10 Class2 0.0976  0.902    Class2   \n# â€¦ with 490 more rows\n\n\nHereâ€™s a confusion matrix:\n\nconf_mat(two_class_example, truth = truth, estimate = predicted)\n\n          Truth\nPrediction Class1 Class2\n    Class1    227     50\n    Class2     31    192\n\n\nSimple accuracy:\n\naccuracy(two_class_example, truth, predicted)\n\n# A tibble: 1 Ã— 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.838\n\n\nWe can calculate the Matthews correlation coefficient (phi coefficient) just as easily:\n\nmcc(two_class_example, truth, predicted)\n\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 mcc     binary         0.677\n\n\nOr the f statistic:\n\nf_meas(two_class_example, truth, predicted)\n\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 f_meas  binary         0.849\n\n\nAll together now!\n\nclassification_metrics <- metric_set(accuracy, mcc, f_meas)\nclassification_metrics(two_class_example, truth = truth, estimate = predicted)\n\n# A tibble: 3 Ã— 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.838\n2 mcc      binary         0.677\n3 f_meas   binary         0.849\n\n\nWhat if the event of interest (i.e.Â the outcome level youâ€™re concerned about) is the second value, like 0/1? You can specify this in the metric:\n\nf_meas(two_class_example, truth, predicted, event_level = \"second\")\n\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 f_meas  binary         0.826\n\n\nEverybody loves ROC curves tooâ€¦ hereâ€™s how it happens in tidymodels:\n\ntwo_class_curve <- roc_curve(two_class_example, truth, Class1)\ntwo_class_curve\n\n# A tibble: 502 Ã— 3\n   .threshold specificity sensitivity\n        <dbl>       <dbl>       <dbl>\n 1 -Inf           0                 1\n 2    1.79e-7     0                 1\n 3    4.50e-6     0.00413           1\n 4    5.81e-6     0.00826           1\n 5    5.92e-6     0.0124            1\n 6    1.22e-5     0.0165            1\n 7    1.40e-5     0.0207            1\n 8    1.43e-5     0.0248            1\n 9    2.38e-5     0.0289            1\n10    3.30e-5     0.0331            1\n# â€¦ with 492 more rows\n\nroc_auc(two_class_example, truth, Class1)\n\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.939\n\n\nRecall that the ROC curce describes how a classifier performs in relation to the discrimination threshold (where you cutoff the probabilities). The X axis shows how false positive rate, and the y axis shows the true positive rate. The identity line shows what a standard model generating random guesses would achieve. A really good model will have a large area under the curve (AUC), because it is correctly guessing true positives at a faster rate than it is guessing false positives:\n\n\n\nROC Curve\n\n\nThereâ€™s an autoplot method for the ROC (though Iâ€™m not sure why youâ€™d not just plot this yourself; itâ€™s like four lines?)\n\nautoplot(two_class_curve)\n\n\n\n\n\ntwo_class_curve %>%\n    ggplot(aes(x=(1-specificity), y = sensitivity)) +\n    geom_line()\n\n\n\n\nItâ€™s 3 lines.\nHereâ€™s a multiclass problem:\n\ndata(hpc_cv)\ntibble(hpc_cv)\n\n# A tibble: 3,467 Ã— 7\n   obs   pred     VF      F       M          L Resample\n   <fct> <fct> <dbl>  <dbl>   <dbl>      <dbl> <chr>   \n 1 VF    VF    0.914 0.0779 0.00848 0.0000199  Fold01  \n 2 VF    VF    0.938 0.0571 0.00482 0.0000101  Fold01  \n 3 VF    VF    0.947 0.0495 0.00316 0.00000500 Fold01  \n 4 VF    VF    0.929 0.0653 0.00579 0.0000156  Fold01  \n 5 VF    VF    0.942 0.0543 0.00381 0.00000729 Fold01  \n 6 VF    VF    0.951 0.0462 0.00272 0.00000384 Fold01  \n 7 VF    VF    0.914 0.0782 0.00767 0.0000354  Fold01  \n 8 VF    VF    0.918 0.0744 0.00726 0.0000157  Fold01  \n 9 VF    VF    0.843 0.128  0.0296  0.000192   Fold01  \n10 VF    VF    0.920 0.0728 0.00703 0.0000147  Fold01  \n# â€¦ with 3,457 more rows\n\n\nThe methods are pretty much identical in Tidymodels:\n\naccuracy(hpc_cv, obs, pred)\n\n# A tibble: 1 Ã— 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy multiclass     0.709\n\nmcc(hpc_cv, obs, pred)\n\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 mcc     multiclass     0.515\n\n\nSensitivity and specificity are not designed for multiclass problems, but can be extended in this case. You can do this in three main ways:\n\nMacro averaging: This computes a set of one-versus-all metrics that are averaged at the end\nMacro weighted averaging: Similar to macro averaging, but weights the score by the number of samples in each class\nMicro averaging: Computes the contribution for each class, then averages them, then computes a single metric for the aggregate\n\nyardstick implements these for us:\n\nsensitivity(hpc_cv, obs, pred, estimator = \"macro\")\n\n# A tibble: 1 Ã— 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 sensitivity macro          0.560\n\nsensitivity(hpc_cv, obs, pred, estimator = \"macro_weighted\")\n\n# A tibble: 1 Ã— 3\n  .metric     .estimator     .estimate\n  <chr>       <chr>              <dbl>\n1 sensitivity macro_weighted     0.709\n\nsensitivity(hpc_cv, obs, pred, estimator = \"micro\")\n\n# A tibble: 1 Ã— 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 sensitivity micro          0.709\n\n\nFor a multiclass ROC, specify all the columns to get a Hand&Till estimate:\n\nroc_auc(hpc_cv, obs, VF, F, M, L)\n\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc hand_till      0.829\n\nroc_auc(hpc_cv, obs, VF, F, M, L, estimator = \"macro_weighted\")\n\n# A tibble: 1 Ã— 3\n  .metric .estimator     .estimate\n  <chr>   <chr>              <dbl>\n1 roc_auc macro_weighted     0.868\n\n\nYou can, naturally, create grouped metrics:\n\nhpc_cv %>% \n  group_by(Resample) %>% \n  accuracy(obs, pred)\n\n# A tibble: 10 Ã— 4\n   Resample .metric  .estimator .estimate\n   <chr>    <chr>    <chr>          <dbl>\n 1 Fold01   accuracy multiclass     0.726\n 2 Fold02   accuracy multiclass     0.712\n 3 Fold03   accuracy multiclass     0.758\n 4 Fold04   accuracy multiclass     0.712\n 5 Fold05   accuracy multiclass     0.712\n 6 Fold06   accuracy multiclass     0.697\n 7 Fold07   accuracy multiclass     0.675\n 8 Fold08   accuracy multiclass     0.721\n 9 Fold09   accuracy multiclass     0.673\n10 Fold10   accuracy multiclass     0.699\n\nhpc_cv %>% \n  group_by(Resample) %>% \n  roc_curve(obs, VF, F, M, L) %>% \n  autoplot()"
  }
]